{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "156c6aac-fb11-4c59-ac01-a9025b4d840d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32000/32000 [01:12<00:00, 443.48it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8000/8000 [00:19<00:00, 419.60it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12000/12000 [00:28<00:00, 422.13it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def get_files(directories):\n",
    "    \"\"\"Returns a list of all files from multiple directories.\"\"\"\n",
    "    files = []\n",
    "    for directory in directories:\n",
    "        files.extend(Path(directory).rglob('*'))\n",
    "    return files\n",
    "\n",
    "def process_file(item, index):\n",
    "    \"\"\"Processes a single file and returns its JSON entry.\"\"\"\n",
    "    try:\n",
    "        return {\n",
    "            'label': item.parts[-2],  # Folder name as label\n",
    "            'index': str(index),\n",
    "            'code': item.read_text(encoding='latin-1')\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {item}: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_files_in_parallel(files):\n",
    "    \"\"\"Processes files in parallel using ThreadPoolExecutor.\"\"\"\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        # Map each file with its corresponding index\n",
    "        data = list(tqdm(\n",
    "            executor.map(lambda p: process_file(p[0], p[1]), zip(files, range(len(files)))), \n",
    "            total=len(files)\n",
    "        ))\n",
    "    return [d for d in data if d is not None]  # Filter out any failed reads\n",
    "\n",
    "def write_jsonl(file_path, data, batch_size=1000):\n",
    "    \"\"\"Writes the given data to a JSONL file in batches.\"\"\"\n",
    "    with open(file_path, 'w') as f:\n",
    "        for i in range(0, len(data), batch_size):\n",
    "            batch = data[i:i + batch_size]\n",
    "            f.writelines(json.dumps(entry) + '\\n' for entry in batch)\n",
    "\n",
    "# Dataset directories\n",
    "train_dirs = [f\"ProgramData/{i}\" for i in range(1, 65)]\n",
    "valid_dirs = [f\"ProgramData/{i}\" for i in range(65, 81)]\n",
    "test_dirs = [f\"ProgramData/{i}\" for i in range(81, 105)]\n",
    "\n",
    "# Process and write datasets\n",
    "train_files = get_files(train_dirs)\n",
    "train_data = process_files_in_parallel(train_files)\n",
    "write_jsonl(\"train.jsonl\", train_data)\n",
    "\n",
    "valid_files = get_files(valid_dirs)\n",
    "valid_data = process_files_in_parallel(valid_files)\n",
    "write_jsonl(\"valid.jsonl\", valid_data)\n",
    "\n",
    "test_files = get_files(test_dirs)\n",
    "test_data = process_files_in_parallel(test_files)\n",
    "write_jsonl(\"test.jsonl\", test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dca9a99-f39f-429c-a2d3-76e7d882d773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b0df2d5dd4344519d48fb68bacff6c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f69e3218ca97448da73594de2ab7a96c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top K Similar Snippets:\n",
      "\n",
      "Snippet 1:\n",
      "int main()\n",
      "{\n",
      "    int i,n,a[400],b[400],j,k,m;\n",
      "\tscanf(\"%d\",&n);\n",
      "\tfor(i=0;i<=n-1;i++)\n",
      "\t{scanf(\"%d\",&a[i]);}\n",
      "    for(m=0,i=0;i<=n-1;i++)\n",
      "\t{\n",
      "\t\tfor(j=0,k=1;j<=i-1;j++)\n",
      "\t\t{if (a[i]==a[j])\n",
      "\t\t{k=0;break;}\n",
      "\t\t}\n",
      "\t\tif(k!=0)\n",
      "\t\t{b[m]=a[i];m++;}\n",
      "\t}\n",
      "\tfor(i=0;i<=m-2;i++)\n",
      "\t{ printf(\"%d,\",b[i]);};\n",
      "\tprintf(\"%d\",b[m-1]);\n",
      "}\n",
      "\n",
      "\n",
      "Similarity Score: 0.9884\n",
      "\n",
      "Snippet 2:\n",
      "\n",
      "int main()\n",
      "{\n",
      "   int n,k,x[1000];\n",
      "   int i,j,sign=0;\n",
      "   \n",
      "   scanf(\"%d%d\",&n,&k);\n",
      "   for(i=0;i<n;i++)\n",
      "   {\n",
      "      scanf(\"%d\",&x[i]);                \n",
      "   }    \n",
      "   \n",
      "   for(i=0;i<n;i++)\n",
      "   {\n",
      "       \n",
      "       for(j=i+1;j<n;j++)\n",
      "       {\n",
      "           if((x[i]+x[j])==k)\n",
      "           {\n",
      "               sign=1;\n",
      "               break;                  \n",
      "           }                  \n",
      "       }\n",
      "                     \n",
      "   }\n",
      "   \n",
      "   if(sign==1)\n",
      "       printf(\"yes\");\n",
      "   else\n",
      "       printf(\"no\");\n",
      "       \n",
      "    scanf(\"%d%d\",&n,&k);   \n",
      "   return 0;\n",
      "}\n",
      "\n",
      "Similarity Score: 0.9879\n",
      "\n",
      "Snippet 3:\n",
      "int main(){\n",
      "    int n,k,i,j,x[1000];\n",
      "    scanf(\"%d %d\",&n,&k);\n",
      "    for(i=0;i<n;i++){\n",
      "           scanf(\"%d\",&x[i]);\n",
      "    }\n",
      "    \n",
      "    for(j=0;j<n;j++){\n",
      "            for(i=j+1;i<=n;i++){\n",
      "                   if(x[j]+x[i]==k){\n",
      "                          printf(\"yes\");\n",
      "                          return 0;\n",
      "                   }\n",
      "            }\n",
      "    }\n",
      "    printf(\"no\");\n",
      "    \n",
      "    scanf(\"%d %d\",&n,&k);                \n",
      "    return 0;\n",
      "}\n",
      "Similarity Score: 0.9847\n",
      "\n",
      "Snippet 4:\n",
      "int main()\n",
      "{\n",
      "  int m,j,i,b[1000];\n",
      "  double a[1000],s=0;\n",
      "  scanf(\"%d\",&m);\n",
      "  for(j=0;j<m;j++){\n",
      "    scanf(\"%d\",&b[j]);\n",
      "  }\n",
      "  for(j=0;j<m;j++){\n",
      "     for(i=0;i<b[j];i++){\n",
      "        a[0]=2/1;\n",
      "        a[i+1]=1+1/a[i];\n",
      "        s+=a[i]; \n",
      "\t }\n",
      "     printf(\"%.3lf\\n\",s);\n",
      "\t s=0;\n",
      "  }\n",
      "  return 0;\n",
      "}\n",
      "\n",
      "\n",
      "Similarity Score: 0.9785\n",
      "\n",
      "Snippet 5:\n",
      "int f(int a,int min)\n",
      "{ \n",
      "\tif(a<min)\n",
      "\t{ \n",
      "\t\treturn 0; \n",
      "\t} \n",
      "\tint result=1,i; \n",
      "\tfor(i=min;i<a;i++)\n",
      "\t{ \n",
      "\t\tif(a%i==0)\n",
      "\t\t{ \n",
      "\t\t\tresult+=f(a/i,i); \n",
      "\t\t} \n",
      "\t} \n",
      "\treturn (result); \n",
      "} \n",
      "int main()\n",
      "{\n",
      "    int n,a[100],b[100],i;\n",
      "\tscanf(\"%d\\n\",&n);\n",
      "    for(i=0;i<n;i++)\n",
      "\t{\n",
      "\t\tscanf(\"%d\\n\",&a[i]);\n",
      "\t}\n",
      "    for(i=0;i<n;i++)\n",
      "\t{\n",
      "\t\tb[i]=f(a[i],2);\n",
      "\t}\n",
      "    for(i=0;i<n;i++)\n",
      "\t{printf(\"%d\\n\",b[i]);}\n",
      "\treturn 0;\n",
      "}\n",
      "\n",
      "Similarity Score: 0.9773\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load the dataset from JSONL file\n",
    "dataset = load_dataset('json', data_files='train.jsonl')['train']\n",
    "\n",
    "# Set a limit to reduce dataset size if needed\n",
    "LIMIT = 100  # Adjust based on available memory and time constraints\n",
    "dataset = dataset.shuffle(seed=42).select(range(min(LIMIT, len(dataset))))\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Use GPU if available for faster processing\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_code(example):\n",
    "    return tokenizer(\n",
    "        example['code'], \n",
    "        padding='max_length', \n",
    "        truncation=True, \n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "# Apply tokenization to the dataset\n",
    "tokenized_dataset = dataset.map(tokenize_code, batched=True)\n",
    "\n",
    "# Extract original code snippets for later use\n",
    "candidate_snippets = [example['code'] for example in dataset]\n",
    "\n",
    "# Function to compute embeddings for a batch of code snippets\n",
    "def get_batch_embeddings(snippets):\n",
    "    inputs = tokenizer(\n",
    "        snippets, return_tensors='pt', \n",
    "        padding=True, truncation=True, max_length=512\n",
    "    )\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    return outputs.last_hidden_state[:, 0, :].cpu().numpy()  # CLS token embeddings\n",
    "\n",
    "# Generate embeddings for all candidate snippets in batches\n",
    "batch_size = 16  # Adjust based on available memory\n",
    "candidate_embeddings = []\n",
    "\n",
    "for i in range(0, len(candidate_snippets), batch_size):\n",
    "    batch = candidate_snippets[i:i + batch_size]\n",
    "    batch_embeddings = get_batch_embeddings(batch)\n",
    "    candidate_embeddings.extend(batch_embeddings)\n",
    "\n",
    "# Convert embeddings to NumPy array for efficient computation\n",
    "candidate_embeddings = np.array(candidate_embeddings)\n",
    "\n",
    "# Function to retrieve top K semantically similar snippets\n",
    "def retrieve_top_k_similar(query_code, K):\n",
    "    query_embedding = get_batch_embeddings([query_code])[0]  # Get embedding for query\n",
    "    similarities = cosine_similarity([query_embedding], candidate_embeddings)[0]  # Compute similarity\n",
    "    \n",
    "    # Get indices of top K similar snippets\n",
    "    top_k_indices = np.argsort(similarities)[-K:][::-1]\n",
    "    top_k_snippets = [candidate_snippets[i] for i in top_k_indices]\n",
    "    top_k_scores = [similarities[i] for i in top_k_indices]\n",
    "    \n",
    "    return top_k_snippets, top_k_scores\n",
    "\n",
    "# Example usage\n",
    "query_code = \"\"\"int main(int argc, char* argv[]) {\n",
    "    int shu[number];\n",
    "    int n, i, j;\n",
    "    int k = 0;\n",
    "    scanf(\"%d\", &shu[0]);\n",
    "    for (n = 0; shu[n] != 0; n++) {\n",
    "        scanf(\"%d\", &shu[n + 1]);\n",
    "    }\n",
    "    for (i = 0; i <= n; i++) {\n",
    "        for (j = 0; j <= n; j++) {\n",
    "            if (shu[i] == 2 * shu[j]) {\n",
    "                k++;\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    if (k != 0) {\n",
    "        k = k - 1;\n",
    "        printf(\"%d\", k);\n",
    "    } else printf(\"%d\", k);\n",
    "    return 0;\n",
    "}\"\"\"  # Your query code snippet\n",
    "\n",
    "K = 5  # Number of top similar snippets to retrieve\n",
    "\n",
    "# Retrieve the top K similar code snippets\n",
    "top_k_snippets, top_k_scores = retrieve_top_k_similar(query_code, K)\n",
    "\n",
    "# Print the results\n",
    "print(\"Top K Similar Snippets:\")\n",
    "for i, snippet in enumerate(top_k_snippets):\n",
    "    print(f\"\\nSnippet {i + 1}:\")\n",
    "    print(snippet)\n",
    "    print(f\"Similarity Score: {top_k_scores[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28043e82-33b5-4fd4-8042-e2fc4dd5de93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cddb3ec790a4c42a8eea38d6ad959c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top K Similar Snippets:\n",
      "\n",
      "Snippet 1:\n",
      "int main(){\n",
      "\tint m=0,k,n,i=0,j=0,a[1000];\n",
      "\tscanf(\"%d %d\",&n,&k);\n",
      "\tfor(i=0;i<n;i++){\n",
      "\t\tscanf(\"%d\",&a[i]);\n",
      "\t}\n",
      "\ti=0;\n",
      "\tfor(j=0;j<n;j++){\n",
      "\t\ti=0;\n",
      "\t\tfor(i=0;i<n;i++){\n",
      "\t\t\tif(i!=j){\n",
      "\t\t\t\tif((a[i]+a[j])==k){\n",
      "\t\t\t\t\tm=1;\n",
      "\t\t\t\t\tbreak;\n",
      "\t\t\t\t}\n",
      "\t\t\t}\n",
      "\t\t}\n",
      "\t}\n",
      "\tif(m==1){printf(\"yes\");}\n",
      "\telse{printf(\"no\");}\n",
      "return 0;\n",
      "}\n",
      "Similarity Score: 0.9891\n",
      "\n",
      "Snippet 2:\n",
      "int main()\n",
      "{\n",
      "    int i,n,a[400],b[400],j,k,m;\n",
      "\tscanf(\"%d\",&n);\n",
      "\tfor(i=0;i<=n-1;i++)\n",
      "\t{scanf(\"%d\",&a[i]);}\n",
      "    for(m=0,i=0;i<=n-1;i++)\n",
      "\t{\n",
      "\t\tfor(j=0,k=1;j<=i-1;j++)\n",
      "\t\t{if (a[i]==a[j])\n",
      "\t\t{k=0;break;}\n",
      "\t\t}\n",
      "\t\tif(k!=0)\n",
      "\t\t{b[m]=a[i];m++;}\n",
      "\t}\n",
      "\tfor(i=0;i<=m-2;i++)\n",
      "\t{ printf(\"%d,\",b[i]);};\n",
      "\tprintf(\"%d\",b[m-1]);\n",
      "}\n",
      "\n",
      "\n",
      "Similarity Score: 0.9884\n",
      "\n",
      "Snippet 3:\n",
      "\n",
      "int main()\n",
      "{\n",
      "   int n,k,x[1000];\n",
      "   int i,j,sign=0;\n",
      "   \n",
      "   scanf(\"%d%d\",&n,&k);\n",
      "   for(i=0;i<n;i++)\n",
      "   {\n",
      "      scanf(\"%d\",&x[i]);                \n",
      "   }    \n",
      "   \n",
      "   for(i=0;i<n;i++)\n",
      "   {\n",
      "       \n",
      "       for(j=i+1;j<n;j++)\n",
      "       {\n",
      "           if((x[i]+x[j])==k)\n",
      "           {\n",
      "               sign=1;\n",
      "               break;                  \n",
      "           }                  \n",
      "       }\n",
      "                     \n",
      "   }\n",
      "   \n",
      "   if(sign==1)\n",
      "       printf(\"yes\");\n",
      "   else\n",
      "       printf(\"no\");\n",
      "       \n",
      "    scanf(\"%d%d\",&n,&k);   \n",
      "   return 0;\n",
      "}\n",
      "\n",
      "Similarity Score: 0.9879\n",
      "\n",
      "Snippet 4:\n",
      "int main()\n",
      "{\n",
      "    int a[20000],n,i,j;\n",
      "    scanf(\"%d\",&n);\n",
      "    for(i=0;i<n;i++)\n",
      "    {\n",
      "        scanf(\"%d\",&a[i]);\n",
      "    }\n",
      "    printf(\"%d\",a[0]);\n",
      "    for(i=1;i<n;i++)\n",
      "    {\n",
      "        for(j=0;j<i;j++)\n",
      "        {\n",
      "            if(a[i]==a[j])\n",
      "            {j=i;}\n",
      "            else\n",
      "            {j=j;}\n",
      "\n",
      "        }\n",
      "        if(j==i)\n",
      "        printf(\",%d\",a[i]);\n",
      "    }\n",
      "    return 0;\n",
      "}\n",
      "Similarity Score: 0.9857\n",
      "\n",
      "Snippet 5:\n",
      "int main()\n",
      "{\n",
      "\tchar s[100],a[100],b[100],d[100][100];\n",
      "\tint i,j,w,m,k=0;\n",
      "\tgets(s);\n",
      "\tgets(a);\n",
      "\tgets(b);\n",
      "\tw=strlen(s);\n",
      "\tfor(i=0,j=0;i<w+1;j++,i++)\n",
      "\t{\n",
      "\t\tif(s[i]!=' '&&s[i]!=0)\n",
      "\t\t{\n",
      "\t\t\td[k][j]=s[i];\n",
      "\t\t}\n",
      "\t\telse\n",
      "\t\t{\n",
      "\t\t\td[k][j]=0;\n",
      "\t\t\tk+=1;\n",
      "\t\t\tj=-1;\n",
      "\t\t}\t\n",
      "\t}\n",
      "    for(i=0;i<k;i++)\n",
      "\t{\n",
      "\t\tif(strcmp(d[i],a)==0)\n",
      "\t\t{\n",
      "\t\t\tstrcpy(d[i],b);\n",
      "\t\t}\n",
      "\t}\n",
      "\tfor(i=0;i<k-1;i++)\n",
      "\t{\n",
      "\t\tprintf(\"%s \",d[i]);\n",
      "\t}\n",
      "\tprintf(\"%s\",d[k-1]);\n",
      "\treturn 0;\n",
      "}\n",
      "Similarity Score: 0.9849\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load the dataset from JSONL file\n",
    "dataset = load_dataset('json', data_files='train.jsonl')['train']\n",
    "\n",
    "# Set a limit to reduce dataset size if needed\n",
    "LIMIT = 200  # Adjust based on available memory and time constraints\n",
    "dataset = dataset.shuffle(seed=42).select(range(min(LIMIT, len(dataset))))\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Use GPU if available for faster processing\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_code(example):\n",
    "    return tokenizer(\n",
    "        example['code'], \n",
    "        padding='max_length', \n",
    "        truncation=True, \n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "# Apply tokenization to the dataset\n",
    "tokenized_dataset = dataset.map(tokenize_code, batched=True)\n",
    "\n",
    "# Extract original code snippets for later use\n",
    "candidate_snippets = [example['code'] for example in dataset]\n",
    "\n",
    "# Function to compute embeddings for a batch of code snippets\n",
    "def get_batch_embeddings(snippets):\n",
    "    inputs = tokenizer(\n",
    "        snippets, return_tensors='pt', \n",
    "        padding=True, truncation=True, max_length=512\n",
    "    )\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    return outputs.last_hidden_state[:, 0, :].cpu().numpy()  # CLS token embeddings\n",
    "\n",
    "# Generate embeddings for all candidate snippets in batches\n",
    "batch_size = 16  # Adjust based on available memory\n",
    "candidate_embeddings = []\n",
    "\n",
    "for i in range(0, len(candidate_snippets), batch_size):\n",
    "    batch = candidate_snippets[i:i + batch_size]\n",
    "    batch_embeddings = get_batch_embeddings(batch)\n",
    "    candidate_embeddings.extend(batch_embeddings)\n",
    "\n",
    "# Convert embeddings to NumPy array for efficient computation\n",
    "candidate_embeddings = np.array(candidate_embeddings)\n",
    "\n",
    "# Function to retrieve top K semantically similar snippets\n",
    "def retrieve_top_k_similar(query_code, K):\n",
    "    query_embedding = get_batch_embeddings([query_code])[0]  # Get embedding for query\n",
    "    similarities = cosine_similarity([query_embedding], candidate_embeddings)[0]  # Compute similarity\n",
    "    \n",
    "    # Get indices of top K similar snippets\n",
    "    top_k_indices = np.argsort(similarities)[-K:][::-1]\n",
    "    top_k_snippets = [candidate_snippets[i] for i in top_k_indices]\n",
    "    top_k_scores = [similarities[i] for i in top_k_indices]\n",
    "    \n",
    "    return top_k_snippets, top_k_scores\n",
    "\n",
    "# Example usage\n",
    "query_code = \"\"\"int main(int argc, char* argv[]) {\n",
    "    int shu[number];\n",
    "    int n, i, j;\n",
    "    int k = 0;\n",
    "    scanf(\"%d\", &shu[0]);\n",
    "    for (n = 0; shu[n] != 0; n++) {\n",
    "        scanf(\"%d\", &shu[n + 1]);\n",
    "    }\n",
    "    for (i = 0; i <= n; i++) {\n",
    "        for (j = 0; j <= n; j++) {\n",
    "            if (shu[i] == 2 * shu[j]) {\n",
    "                k++;\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    if (k != 0) {\n",
    "        k = k - 1;\n",
    "        printf(\"%d\", k);\n",
    "    } else printf(\"%d\", k);\n",
    "    return 0;\n",
    "}\"\"\"  # Your query code snippet\n",
    "\n",
    "K = 5  # Number of top similar snippets to retrieve\n",
    "\n",
    "# Retrieve the top K similar code snippets\n",
    "top_k_snippets, top_k_scores = retrieve_top_k_similar(query_code, K)\n",
    "\n",
    "# Print the results\n",
    "print(\"Top K Similar Snippets:\")\n",
    "for i, snippet in enumerate(top_k_snippets):\n",
    "    print(f\"\\nSnippet {i + 1}:\")\n",
    "    print(snippet)\n",
    "    print(f\"Similarity Score: {top_k_scores[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90d845f-0852-460f-9bdb-bf8fb0ef6735",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\purva\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='319' max='12000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  319/12000 5:27:33 < 201:10:19, 0.02 it/s, Epoch 0.08/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Load dataset from jsonl\n",
    "train_dataset = load_dataset('json', data_files='train.jsonl', split='train')\n",
    "valid_dataset = load_dataset('json', data_files='valid.jsonl', split='train')\n",
    "\n",
    "# Load tokenizer (CodeBERT or GraphCodeBERT)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "# Tokenize dataset (adjust max_length to 512)\n",
    "def tokenize_code(example):\n",
    "    return tokenizer(example['code'], padding='max_length', truncation=True, max_length=512)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_code, batched=True)\n",
    "valid_dataset = valid_dataset.map(tokenize_code, batched=True)\n",
    "\n",
    "# Convert labels to integers\n",
    "def process_labels(example):\n",
    "    example['label'] = int(example['label'])  # Convert label to an integer\n",
    "    return example\n",
    "\n",
    "# Apply the function to both train and validation datasets\n",
    "train_dataset = train_dataset.map(process_labels)\n",
    "valid_dataset = valid_dataset.map(process_labels)\n",
    "\n",
    "# Load the model\n",
    "#codebert_model = RobertaForSequenceClassification.from_pretrained(\"microsoft/codebert-base\", num_labels=65)\n",
    "codebert_model = RobertaForSequenceClassification.from_pretrained(\"microsoft/codebert-base\", num_labels=81)\n",
    "\n",
    "# Training setup\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=codebert_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset\n",
    ")\n",
    "\n",
    "# Fine-tuning\n",
    "trainer.train()\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model_save_path = './fine_tuned_robertamodel'\n",
    "codebert_model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0a7a17-6ee7-4efd-9eee-41c348935b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\purva\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='12000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  320/12000 6:47:56 < 249:43:21, 0.01 it/s, Epoch 0.08/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This is for BERT model\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Step 1: Load datasets for fine-tuning\n",
    "train_dataset = load_dataset('json', data_files='train.jsonl')['train']\n",
    "valid_dataset = load_dataset('json', data_files='valid.jsonl')['train']\n",
    "test_dataset = load_dataset('json', data_files='test.jsonl')['train']\n",
    "\n",
    "\n",
    "def process_labels(example):\n",
    "    example['label'] = int(example['label'])  # Convert label from string to integer\n",
    "    return example\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(process_labels)\n",
    "valid_dataset = valid_dataset.map(process_labels)\n",
    "test_dataset = test_dataset.map(process_labels)\n",
    "\n",
    "\n",
    "# Load BERT tokenizer and pre-trained BERT model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=81)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Tokenize datasets\n",
    "def tokenize_code(example):\n",
    "    return tokenizer(\n",
    "        example['code'], \n",
    "        padding='max_length', \n",
    "        truncation=True, \n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_code, batched=True)\n",
    "valid_dataset = valid_dataset.map(tokenize_code, batched=True)\n",
    "\n",
    "# Define TrainingArguments and Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_steps=10_000,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Step 2: Embedding generation for similarity retrieval\n",
    "# Load candidate snippets from candidates.json\n",
    "candidate_dataset = load_dataset('json', data_files='candidates.json')['train']\n",
    "candidate_snippets = [example['code'] for example in candidate_dataset]\n",
    "\n",
    "# Function to compute embeddings for a batch of code snippets\n",
    "def get_batch_embeddings(snippets):\n",
    "    inputs = tokenizer(\n",
    "        snippets, return_tensors='pt', \n",
    "        padding=True, truncation=True, max_length=512\n",
    "    )\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.bert(**inputs)  # Directly access BERT part of model\n",
    "    return outputs.last_hidden_state[:, 0, :].cpu().numpy()  # CLS token embeddings\n",
    "\n",
    "# Generate embeddings for candidate snippets in batches\n",
    "batch_size = 16\n",
    "candidate_embeddings = []\n",
    "\n",
    "for i in range(0, len(candidate_snippets), batch_size):\n",
    "    batch = candidate_snippets[i:i + batch_size]\n",
    "    batch_embeddings = get_batch_embeddings(batch)\n",
    "    candidate_embeddings.extend(batch_embeddings)\n",
    "\n",
    "# Convert embeddings to a NumPy array for cosine similarity calculation\n",
    "candidate_embeddings = np.array(candidate_embeddings)\n",
    "\n",
    "# Function to retrieve top K semantically similar snippets\n",
    "def retrieve_top_k_similar(query_code, K):\n",
    "    query_embedding = get_batch_embeddings([query_code])[0]  # Get embedding for query\n",
    "    similarities = cosine_similarity([query_embedding], candidate_embeddings)[0]  # Compute similarity\n",
    "    \n",
    "    # Get indices of top K similar snippets\n",
    "    top_k_indices = np.argsort(similarities)[-K:][::-1]\n",
    "    top_k_snippets = [candidate_snippets[i] for i in top_k_indices]\n",
    "    top_k_scores = [similarities[i] for i in top_k_indices]\n",
    "    \n",
    "    return top_k_snippets, top_k_scores\n",
    "\n",
    "# Example usage with a query code snippet\n",
    "query_code = \"\"\"// Sample query code to find top-K similar snippets\n",
    "int sumArray(int arr[], int size) {\n",
    "    int sum = 0;\n",
    "    for (int i = 0; i < size; i++) {\n",
    "        sum += arr[i];\n",
    "    }\n",
    "    return sum;\n",
    "}\"\"\"\n",
    "K = 5\n",
    "\n",
    "top_k_snippets, top_k_scores = retrieve_top_k_similar(query_code, K)\n",
    "\n",
    "# Print results\n",
    "print(\"Top K Similar Snippets:\")\n",
    "for i, snippet in enumerate(top_k_snippets):\n",
    "    print(f\"\\nSnippet {i + 1}:\")\n",
    "    print(snippet)\n",
    "    print(f\"Similarity Score: {top_k_scores[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b42d35ad-3685-4935-85b1-282568b49f36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6bbf2e954234383a9008ebceb98b957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\purva\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\purva\\.cache\\huggingface\\hub\\models--roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5571b90fad842da8cb30f9afbcb33e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdc95bfae6cc4de6b0098a853c5c704f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1c3ae1aeb354b5f9c6e20e8c253e91a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fe421a88cf54fc7b018b9481fe9eafa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fd83e23543f435baad151b3aa2b0f65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "515dfacd5489414bb2dc5f9b285f418a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d4d24cf2b07414599dbe71f4dcac3c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\purva\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 03:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.549133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.469843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.888752</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top K Similar Snippets:\n",
      "\n",
      "Snippet 1:\n",
      "int binarySearch(int arr[], int left, int right, int x) { while(left <= right) { int mid = left + (right - left) / 2; if(arr[mid] == x) return mid; if(arr[mid] < x) left = mid + 1; else right = mid - 1; } return -1; }\n",
      "Similarity Score: 0.9726\n",
      "\n",
      "Snippet 2:\n",
      "void sortArray(int arr[], int size) { for(int i = 0; i < size - 1; i++) { for(int j = 0; j < size - i - 1; j++) { if(arr[j] > arr[j + 1]) { int temp = arr[j]; arr[j] = arr[j + 1]; arr[j + 1] = temp; } } } }\n",
      "Similarity Score: 0.9697\n",
      "\n",
      "Snippet 3:\n",
      "void reverseArray(int arr[], int size) { int start = 0, end = size - 1; while(start < end) { int temp = arr[start]; arr[start] = arr[end]; arr[end] = temp; start++; end--; } }\n",
      "Similarity Score: 0.9667\n",
      "\n",
      "Snippet 4:\n",
      "int findMax(int arr[], int size) { int max = arr[0]; for(int i = 1; i < size; i++) { if(arr[i] > max) { max = arr[i]; } } return max; }\n",
      "Similarity Score: 0.9572\n",
      "\n",
      "Snippet 5:\n",
      "void printArray(int arr[], int size) { for(int i = 0; i < size; i++) { printf(\"%d \", arr[i]); } }\n",
      "Similarity Score: 0.9503\n"
     ]
    }
   ],
   "source": [
    "# This is for Roberta Model\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Step 1: Load datasets for fine-tuning\n",
    "train_dataset = load_dataset('json', data_files='train_copy.jsonl')['train']\n",
    "valid_dataset = load_dataset('json', data_files='valid_copy.jsonl')['train']\n",
    "test_dataset = load_dataset('json', data_files='test_copy.jsonl')['train']\n",
    "\n",
    "def process_labels(example):\n",
    "    example['label'] = int(example['label'])  # Convert label from string to integer\n",
    "    return example\n",
    "\n",
    "train_dataset = train_dataset.map(process_labels)\n",
    "valid_dataset = valid_dataset.map(process_labels)\n",
    "test_dataset = test_dataset.map(process_labels)\n",
    "\n",
    "# Load RoBERTa tokenizer and pre-trained RoBERTa model\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=81)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Tokenize datasets\n",
    "def tokenize_code(example):\n",
    "    return tokenizer(\n",
    "        example['code'], \n",
    "        padding='max_length', \n",
    "        truncation=True, \n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_code, batched=True)\n",
    "valid_dataset = valid_dataset.map(tokenize_code, batched=True)\n",
    "\n",
    "# Define TrainingArguments and Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_steps=10_000,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Step 2: Embedding generation for similarity retrieval\n",
    "# Load candidate snippets from candidates.json\n",
    "candidate_dataset = load_dataset('json', data_files='candidates.json')['train']\n",
    "candidate_snippets = [example['code'] for example in candidate_dataset]\n",
    "\n",
    "# Function to compute embeddings for a batch of code snippets\n",
    "def get_batch_embeddings(snippets):\n",
    "    inputs = tokenizer(\n",
    "        snippets, return_tensors='pt', \n",
    "        padding=True, truncation=True, max_length=512\n",
    "    )\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.roberta(**inputs)  # Directly access RoBERTa part of model\n",
    "    return outputs.last_hidden_state[:, 0, :].cpu().numpy()  # CLS token embeddings\n",
    "\n",
    "# Generate embeddings for candidate snippets in batches\n",
    "batch_size = 16\n",
    "candidate_embeddings = []\n",
    "\n",
    "for i in range(0, len(candidate_snippets), batch_size):\n",
    "    batch = candidate_snippets[i:i + batch_size]\n",
    "    batch_embeddings = get_batch_embeddings(batch)\n",
    "    candidate_embeddings.extend(batch_embeddings)\n",
    "\n",
    "# Convert embeddings to a NumPy array for cosine similarity calculation\n",
    "candidate_embeddings = np.array(candidate_embeddings)\n",
    "\n",
    "# Function to retrieve top K semantically similar snippets\n",
    "def retrieve_top_k_similar(query_code, K):\n",
    "    query_embedding = get_batch_embeddings([query_code])[0]  # Get embedding for query\n",
    "    similarities = cosine_similarity([query_embedding], candidate_embeddings)[0]  # Compute similarity\n",
    "    \n",
    "    # Get indices of top K similar snippets\n",
    "    top_k_indices = np.argsort(similarities)[-K:][::-1]\n",
    "    top_k_snippets = [candidate_snippets[i] for i in top_k_indices]\n",
    "    top_k_scores = [similarities[i] for i in top_k_indices]\n",
    "    \n",
    "    return top_k_snippets, top_k_scores\n",
    "\n",
    "# Example usage with a query code snippet\n",
    "query_code = \"\"\"// Sample query code to find top-K similar snippets\n",
    "int sumArray(int arr[], int size) {\n",
    "    int sum = 0;\n",
    "    for (int i = 0; i < size; i++) {\n",
    "        sum += arr[i];\n",
    "    }\n",
    "    return sum;\n",
    "}\"\"\"\n",
    "K = 5\n",
    "\n",
    "top_k_snippets, top_k_scores = retrieve_top_k_similar(query_code, K)\n",
    "\n",
    "# Print results\n",
    "print(\"Top K Similar Snippets:\")\n",
    "for i, snippet in enumerate(top_k_snippets):\n",
    "    print(f\"\\nSnippet {i + 1}:\")\n",
    "    print(snippet)\n",
    "    print(f\"Similarity Score: {top_k_scores[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931130bf-b7ca-46c0-be94-030d7b82e99d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
